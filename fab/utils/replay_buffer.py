from typing import NamedTuple, Tuple, Iterable, Callable
import torch

class AISData(NamedTuple):
    """Log weights and samples generated by annealed importance sampling."""
    x: torch.Tensor
    log_w: torch.Tensor


class ReplayBuffer:
    def __init__(self, dim: int,
                 max_length: int,
                 min_sample_length: int,
                 initial_sampler: Callable[[], Tuple[torch.Tensor, torch.Tensor]],
                 device: str = "cpu",
                 ):
        """
        Create replay buffer for batched sampling and adding of data.
        Args:
            dim: dimension of x data
            max_length: maximum length of the buffer
            min_sample_length: minimum length of buffer required for sampling
            initial_sampler: sampler producing x and log_w, used to fill the buffer up to
                the min sample length
            device: replay buffer device
        """
        self.dim = dim
        self.max_length = max_length
        self.min_sample_length = min_sample_length
        self.buffer = AISData(x=torch.zeros(self.max_length, dim).to(device),
                              log_w=torch.zeros(self.max_length, ).to(device))
        self.possible_indices = torch.arange(self.max_length).to(device)
        self.device = device
        self.current_index = 0
        self.is_full = False  # whether the buffer is full
        self.can_sample = False  # whether the buffer is full enough to begin sampling

        while self.can_sample is False:
            # fill buffer up minimum length
            x, log_w = initial_sampler()
            self.add(x, log_w)

    @torch.no_grad()
    def add(self, x: torch.Tensor, log_w: torch.Tensor):
        batch_size = x.shape[0]
        x = x.to(self.device)
        log_w = log_w.to(self.device)
        indices = (torch.arange(batch_size) + self.current_index).to(self.device) % self.max_length
        self.buffer.x[indices] = x
        self.buffer.log_w[indices] = log_w
        new_index = self.current_index + batch_size
        if not self.is_full:
            self.is_full = new_index >= self.max_length
            self.can_sample = new_index >= self.min_sample_length
        self.current_index = new_index % self.max_length

    @torch.no_grad()
    def sample(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor]:
        """Return a batch of sampled data, if the batch size is specified then the batch will have a
        leading axis of length batch_size, otherwise the default self.batch_size will be used."""
        if not self.can_sample:
            raise Exception("Buffer must be at minimum length before calling sample")
        max_index = self.max_length if self.is_full else self.current_index
        indices = torch.multinomial(torch.arange(max_index, dtype=float), num_samples=batch_size,
                                    replacement=False).to(self.device)
        return self.buffer.x[indices], self.buffer.log_w[indices]


    def sample_n_batches(self, batch_size: int, n_batches: int) -> \
            Iterable[Tuple[torch.Tensor, torch.Tensor]]:
        """Returns a list of batches."""
        x, log_w = self.sample(batch_size*n_batches)
        x_batches = x.split(n_batches)
        log_w_batches = log_w.split(n_batches)
        return [(x, log_w) for x, log_w in zip(x_batches, log_w_batches)]



if __name__ == '__main__':
    dim = 5
    batch_size = 3
    n_batches_total_length = 2
    length = n_batches_total_length * batch_size
    min_sample_length = int(length * 0.5)
    initial_sampler = lambda: (torch.ones(batch_size, dim), torch.zeros(batch_size))
    buffer = ReplayBuffer(dim, length, min_sample_length, initial_sampler)
    n_batches = 3
    for i in range(100):
        buffer.add(torch.ones(batch_size, dim), torch.zeros(batch_size))
        batch = buffer.sample(batch_size)

