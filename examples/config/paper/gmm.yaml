# Currently with fake values so we can run locally with ease
target:
  dim: 2
  loc_scaling: 40
  n_mixes: 40
  log_var_scaling: 1.0

flow:
  layer_nodes_per_dim: 10
  n_layers: 10

fab:
  # ["alpha_2_div", "forward_kl", "sample_log_prob",
  # "flow_forward_kl", "flow_alpha_2_div", "flow_reverse_kl", "p2_over_q_alpha_2_div"]
  loss_type: "p2_over_q_alpha_2_div"
  transition_operator:
    type: hmc
    n_inner_steps: 5
  n_intermediate_distributions: 2


training:
  seed: 0
  lr: 2e-3
  batch_size: 64
  n_iterations: 100
  use_gpu: true
  use_64_bit: false
  use_buffer: false # below config fields are all for use_buffer = True
  prioritised_buffer: true
  n_batches_buffer_sampling: 2
  buffer_temp: 1.0 # rate that we weight new experience over old
  maximum_buffer_length: 1000
  min_buffer_length: 64 # heuristic: set this to n_batches_buffer_sampling*batch_size*10
  log_w_clip_frac: null # null for no clipping, for non-prioritised replay
  max_grad_norm: 10 # null for no clipping
  w_adjust_max_clip: 10.0 # clipping of weight adjustment factor for prioritised replay


evaluation:
  n_plots: 10 # number of times we visualise the model throughout training.
  n_eval: 5 # for calculating metrics of flow w.r.t target.
  eval_batch_size: 128 # must be a multiple of inner batch size
  n_checkpoints: 10 # number of model checkpoints saved
  save_path:  results/many_well32/


logger:
  list_logger: true
#  pandas_logger:
#    save_period: 100 # how often to save the pandas dataframe as a csv
#  wandb:
#    name: ManyWell32
#    tags: [alpha_2_loss,ManyWell32]

