defaults:
  - override hydra/launcher: joblib

target:
  dim: 2
  loc_scaling: 40
  n_mixes: 40
  log_var_scaling: 1.0

flow:
  use_snf: false
  layer_nodes_per_dim: 30
  n_layers: 10
  act_norm: false

fab:
  loss_type: p2_over_q_alpha_2_div # alpha_2_div # flow_reverse_kl
  transition_operator:
    type: metropolis
    n_inner_steps: 1
  n_intermediate_distributions: 1


training:
  tlimit: null # time limit in hours
  # if checkpoint_load_dir in standard format then will load most recent, null for no loading
  checkpoint_load_dir: null# outer directory for checkpoints, most recent is loaded
  seed: 0
  lr: 1e-4
  batch_size: 128
  n_iterations: null # either n_itertations or n_flow_forward_pass must have a value
  n_flow_forward_pass: 100_000 # assumes split coupling where log prob & sample are same speed.
  use_gpu: true
  use_64_bit: true
  use_buffer: false # below config fields are all for use_buffer = True
  prioritised_buffer: false
  n_batches_buffer_sampling: 4
  buffer_temp: 1.0 # rate that we weight new experience over old
  maximum_buffer_length: 12800
  min_buffer_length: 512 # heuristic: set this to n_batches_buffer_sampling*batch_size*10
  log_w_clip_frac: null # null for no clipping, for non-prioritised replay
  max_grad_norm: 100.0 # null for no clipping
  w_adjust_max_clip: null # clipping of weight adjustment factor for prioritised replay


evaluation:
  n_plots: 20 # number of times we visualise the model throughout training.
  n_eval: 20 # for calculating metrics of flow w.r.t target.list(np.linspace(0, n_iterations - 1, n_checkpoints, dtype="int"))
  eval_batch_size: 512 # must be a multiple of inner batch size
  n_checkpoints: 10 # number of model checkpoints saved
  save_path:  /home/laurence/work/code/FAB-TORCH/results/gmm/seed${training.seed}/snf/


logger:
#  list_logger: true
  pandas_logger:
    save_period: 100 # how often to save the pandas dataframe as a csv


