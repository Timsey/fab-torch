{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd25370",
   "metadata": {},
   "source": [
    "# Visualise AIS\n",
    "In this notebook we perform some visualisations of the annealed sampling algorithms performance, such as how AIS scales with the number of intermediate distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb056a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57471d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from fab.sampling_methods import AnnealedImportanceSampler, Metropolis, HamiltoneanMonteCarlo\n",
    "from fab.utils.logging import ListLogger\n",
    "from fab.target_distributions import TargetDistribution\n",
    "from fab.target_distributions.gmm import GMM\n",
    "from fab.wrappers.torch import WrappedTorchDist\n",
    "from fab.utils.plotting import plot_history, plot_contours, plot_marginal_pair\n",
    "from fab.utils.numerical import effective_sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8585fc4c",
   "metadata": {},
   "source": [
    "## Setup Target Distribution & AIS based distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8c304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim: int = 2\n",
    "seed: int = 1\n",
    "transition_operator_type: str = \"hmc\"\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cbf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = GMM(dim=dim, n_mixes=4, loc_scaling=8)\n",
    "base_dist = WrappedTorchDist(torch.distributions.MultivariateNormal(loc=torch.zeros(dim),\n",
    "                                                                 scale_tril=15*torch.eye(dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target\n",
    "plot_contours(target.log_prob, bounds=[-20, 20], n_contour_levels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedb02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot base distribution\n",
    "base_samples = base_dist.sample((500,))\n",
    "plot_marginal_pair(base_samples, bounds=[-40, 40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cbbcde",
   "metadata": {},
   "source": [
    "## Setup example of AIS\n",
    "First we run look at the effect of tuning the step size for a fixed number of intermediate distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d408902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ais_dist = 5 \n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b32a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_ais(n_ais_intermediate_distributions, transition_operator_type,\n",
    "             step_size_init=1.0, n_outer=5):\n",
    "    if transition_operator_type == \"hmc\":\n",
    "        transition_operator = HamiltoneanMonteCarlo(\n",
    "            n_ais_intermediate_distributions=n_ais_intermediate_distributions,\n",
    "            n_outer=n_outer,\n",
    "            epsilon=step_size_init, L=5, dim=dim,\n",
    "            step_tuning_method=\"p_accept\") # other tuning options include \"No-U\" and \"Expected_target_prob\"\n",
    "    elif transition_operator_type == \"metropolis\":\n",
    "        transition_operator = Metropolis(n_transitions=n_ais_intermediate_distributions,\n",
    "                                         n_updates=5)\n",
    "    ais = AnnealedImportanceSampler(base_distribution=base_dist,\n",
    "                                    target_log_prob=target.log_prob,\n",
    "                                    transition_operator=transition_operator,\n",
    "                                    n_intermediate_distributions=n_ais_intermediate_distributions,\n",
    "                                    )\n",
    "    return ais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2dd45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we give epsilon a poor initialisation so we can visualise the effect of tuning easily\n",
    "ais = setup_ais(n_ais_dist, \"hmc\", step_size_init=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa62d979",
   "metadata": {},
   "source": [
    "### Visualise samples before HMC has been tuned. \n",
    "Note that we have given epsilone a poor initialisation (too big)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f5094",
   "metadata": {},
   "outputs": [],
   "source": [
    "ais.transition_operator.set_eval_mode(True) # turn off tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c2af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, log_w = ais.sample_and_log_weights(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ea10c3",
   "metadata": {},
   "source": [
    "plot the ais samples vs the target probability density contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b87692e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_contours(target.log_prob, ax=ax, bounds=[-30, 30], n_contour_levels=50)\n",
    "plot_marginal_pair(samples, ax=ax, bounds=[-30, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4e08e",
   "metadata": {},
   "source": [
    "Histogram of the log weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(log_w.numpy(), density=True, alpha=0.75, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709aa5c",
   "metadata": {},
   "source": [
    "Histogram of the biggest log w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a7cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(log_w[log_w > -10].numpy(), density=True, alpha=0.75, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca81b5de",
   "metadata": {},
   "source": [
    "### Tune HMC and visualise again.\n",
    "We see that the effective sample size (after ais) goes up during the tuning), and that the samples match the target more closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f647f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ListLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a126619",
   "metadata": {},
   "outputs": [],
   "source": [
    "ais.transition_operator.set_eval_mode(False) # turn on tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(ais, outer_batch_size, inner_batch_size):\n",
    "    ais.transition_operator.set_eval_mode(True) # turn off tuning for evaluation.\n",
    "    base_samples, base_log_w, ais_samples, ais_log_w = \\\n",
    "        ais.generate_eval_data(outer_batch_size, inner_batch_size)\n",
    "    info = {\"eval_ess_base\": effective_sample_size(log_w=base_log_w, normalised=False).item(),\n",
    "            \"eval_ess_ais\": effective_sample_size(log_w=ais_log_w, normalised=False).item()}\n",
    "    ais.transition_operator.set_eval_mode(False) # turn on tuning\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(100)):\n",
    "    samples, log_w = ais.sample_and_log_weights(batch_size)\n",
    "    logging_info = ais.get_logging_info()\n",
    "    logger.write(logging_info)\n",
    "    if i % 10 == 0:\n",
    "        eval_info = eval(ais, 20*batch_size, batch_size)\n",
    "        logger.write(eval_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f711d",
   "metadata": {},
   "source": [
    "In the below plot (in comparison to before HMC was tuned), we see that the points generated by AIS are much closer to the target distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_contours(target.log_prob, ax=ax, bounds=[-30, 30], n_contour_levels=50)\n",
    "plot_marginal_pair(samples, ax=ax, bounds=[-30, 30])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5d7cf",
   "metadata": {},
   "source": [
    "In the history, we see that the step size is decreased to increase the number of accepted HMC trajectories, this (on aggregate) increases the effective sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1832815e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(logger.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e2008",
   "metadata": {},
   "source": [
    "If we compare the below plot of the log weights we see that the width of the distribution is lower, and their is less mass at the tails of the distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0656826f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(log_w.numpy(), density=True, alpha=0.75, bins=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e0ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(log_w[log_w > -10].numpy(), density=True, alpha=0.75, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc20770",
   "metadata": {},
   "source": [
    "## Visualise the effect of the number of AIS distributions\n",
    "We see that as the number of AIS distributions increases, the effective sample size increases, and the variance in the importance log weights decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3171c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "range_n_distributions = [2, 4, 8, 16, 32, 64, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af279c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ListLogger()\n",
    "log_weight_hist = [] # listlogger is meant for scalars so we store the log weight history separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3909e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ess_hist = []\n",
    "for n_ais_dist in tqdm(range_n_distributions):\n",
    "    # turn off step size tuning, initial step size is reasonable and we only want to visualise the effect of \n",
    "    # the number of ais distributions. \n",
    "    ais = setup_ais(n_ais_dist, \"hmc\", step_size_init=1.0, n_outer=1)\n",
    "    ais.transition_operator.set_eval_mode(True) \n",
    "    base_samples, base_log_w, ais_samples, ais_log_w = \\\n",
    "        ais.generate_eval_data(50*batch_size, batch_size)\n",
    "    info = {\"eval_ess_ais\": effective_sample_size(log_w=ais_log_w, normalised=False).item(),\n",
    "           \"log_w_var\": torch.var(ais_log_w).item()}\n",
    "    logger.write(info)\n",
    "    log_weight_hist.append(ais_log_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f94ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2)\n",
    "axs[0].plot(range_n_distributions, logger.history[\"eval_ess_ais\"])\n",
    "axs[0].set_ylabel(\"effective sample size\")\n",
    "axs[0].set_xlabel(\"number of intermediate ais distributions\")\n",
    "\n",
    "axs[1].plot(range_n_distributions, logger.history[\"log_w_var\"])\n",
    "axs[1].set_ylabel(\"var log w\")\n",
    "axs[1].set_xlabel(\"number of intermediate ais distributions\")\n",
    "axs[1].set_yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66efd21c",
   "metadata": {},
   "source": [
    "The log variance initialy decreases by a huge amount, however as the number of AIS distributions increases, the log variance decreases more closely to a rate of 1/n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ea02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.history['log_w_var']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eefd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at samples after a the max number of AIS steps. \n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plot_contours(target.log_prob, ax=axs[0], bounds=[-30, 30], n_contour_levels=50)\n",
    "plot_marginal_pair(ais_samples[:1000], ax=axs[0], bounds=[-30, 30])\n",
    "axs[0].set_title(\"samples (ais) vs target contours\")\n",
    "\n",
    "plot_contours(target.log_prob, ax=axs[1], bounds=[-30, 30], n_contour_levels=50)\n",
    "plot_marginal_pair(target.sample((1000,)), ax=axs[1], bounds=[-30, 30])\n",
    "axs[1].set_title(\"samples (target) vs target contours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb8e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and for comparison with only a few intermediate distributions\n",
    "n_ais_dist = 4 # change this number to see how the number of distributions effects the samples from AIS.\n",
    "ais_2_dist = setup_ais(n_ais_dist, \"hmc\", step_size_init=1.0, n_outer=1)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "plot_contours(target.log_prob, ax=axs[0], bounds=[-30, 30], n_contour_levels=50)\n",
    "plot_marginal_pair(ais_2_dist.sample_and_log_weights(batch_size)[0], ax=axs[0], bounds=[-30, 30])\n",
    "axs[0].set_title(\"samples (ais) vs target contours\")\n",
    "\n",
    "plot_contours(target.log_prob, ax=axs[1], bounds=[-30, 30], n_contour_levels=50)\n",
    "plot_marginal_pair(target.sample((1000,)), ax=axs[1], bounds=[-30, 30])\n",
    "axs[1].set_title(\"samples (target) vs target contours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a36b5",
   "metadata": {},
   "source": [
    "Plot log weight distribution for a relatively low number of AIS distributions vs a high number of AIS distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c767274",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_n_low = 2\n",
    "print(f\"plotting log_w for {range_n_distributions[iter_n_low]} AIS distributions for first 100 samples\")\n",
    "log_w_low = log_weight_hist[iter_n_low][:1000].numpy()\n",
    "n, bins, patches = plt.hist(log_w_low, density=True, alpha=0.75, bins=40, color=\"green\", \n",
    "                            label=f\"{range_n_distributions[iter_n_low]} ais dist\")\n",
    "\n",
    "iter_n_high = -1\n",
    "print(f\"plotting log_w for {range_n_distributions[iter_n_high]} AIS distributions for first 100 samples\")\n",
    "log_w_high = log_weight_hist[iter_n_high][:1000].numpy()\n",
    "n, bins, patches = plt.hist(log_w_high, density=True, alpha=0.75, bins=40, color=\"blue\", \n",
    "                            label=f\"{range_n_distributions[iter_n_high]} ais dist\")\n",
    "\n",
    "plt.xscale(\"symlog\") # use log x scale so we can see both on the same plot\n",
    "plt.legend()\n",
    "plt.xlabel(\"log_w\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650c06b4",
   "metadata": {},
   "source": [
    "The same but dropping very low values for the log weights so we don't need to log the x-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_w_low = log_weight_hist[iter_n_low]\n",
    "log_w_low = log_w_low[log_w_low > -10][:1000].numpy()\n",
    "n, bins, patches = plt.hist(log_w_low, density=True, alpha=0.75, bins=40, color=\"green\", \n",
    "                            label=f\"{range_n_distributions[iter_n_low]} ais dist\")\n",
    "\n",
    "log_w_high = log_weight_hist[iter_n_high]\n",
    "log_w_high = log_w_high[log_w_high > -10][:1000].numpy()\n",
    "n, bins, patches = plt.hist(log_w_high, density=True, alpha=0.75, bins=40, color=\"blue\", \n",
    "                            label=f\"{range_n_distributions[iter_n_high]} ais dist\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"log_w\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68e4ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}