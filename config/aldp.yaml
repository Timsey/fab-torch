# Config file specifying the setup of a Boltzmann Generator


data:
  transform: datasets/boltzmann-generators/aldp_train.h5
  test: datasets/boltzmann-generators/aldp_test_transformed.pt


system:                             # Properties of molecular system

  temperature: 1000                 # Double, temperature of the system
  energy_cut: 1.e+8                 # Double, energy level at which regularization shall be applied
  energy_max: 1.e+20                # Double, maximum level at which energies will be clamped
  n_threads: 4                      # Int, number of threads to be used, number of cores if null


flow:                               # Properties of the flow model

  type: rnvp                        # String, type of the flow
  base:                             # Base distribution
    type: gauss                     # Type of the base dist
    learn_mean_var: True            # Flag, if true, mean and variance of the dist is learned
    params: null
  blocks: 16                        # Int, number of Real NVP blocks, consisting of an ActNorm layer
                                    # if specified, a permutation, and a affine coupling layer
  actnorm: True                     # Bool, flag whether to include an ActNorm layers
  mixing: affine                    # String, how features are mixed
  coupling: affine                  # String, can be affine or additive
  scale_map: sigmoid
  hidden_layers: 2                  # Int, number of hidden layers of the NN in the coupling layers
  hidden_units: 64                  # Int, number of hidden units of the NN in the coupling layers
  init_zeros: True                  # Bool, flag whether to initialize last layer of NNs with zeros


fab:

  transition_type: hmc              # String, type of transition operator used
  n_int_dist: 5                     # Int, number of intermediate distributions



training:                           # Properties of the training procedure
  max_iter: 500000                  # Int, maximum number of iteration
  optimizer: adam                   # String, name of the optimizer
  warmup_iter: 100                  # Int, number of iterations of linearly warm up learning rate
  batch_size: 512                   # Int, batch size used during training
  learning_rate: 1.e-3              # Double, learning rate used during training
  rate_decay: 0.1                   # Double, learning rate decay factor
  decay_iter: 250000                # Int, number of iteration after which the learning rate should be
                                    # decreased
  weight_decay: 1.e-5               # Double, regularization parameter
  log_iter: 1000                    # Int, number of iterations after which loss is saved
  checkpoint_iter: 5000             # Int, number of iterations after which checkpoint is saved
  eval_samples: 1000000             # Int, number of samples to draw when evaluating the model
  seed: 0                           # Int, seed to be used for the random number generator
  save_root: out