# Config file specifying the setup of a Boltzmann Generator

target:                               # Properties of molecular system
  solute: water
  solvent: water
  dim: 45                             # System dimension: 3 * (3 + 3 * num_solvent_molecules) for water in water (xyz).
  temperature: 300                    # Double, temperature of the system
  energy_cut: 1.e+8                   # Double, energy level at which regularization shall be applied
  energy_max: 1.e+20                  # Double, maximum level at which energies will be clamped
  n_threads: 18  #18                   # Int, number of threads to be used, number of cores if null
  solvent_pdb_path: /home/timsey/HDD/data/molecules/solvents/water.pdb    # Filepath to solvent pdb
  train_samples_path: /home/timsey/HDD/data/molecules/md/output_dim${target.dim}_temp${target.temperature}_eq1000_burn100000_steps1000_every10.h5  # Filepath to load samples from
  val_samples_path: /home/timsey/HDD/data/molecules/md/output_dim${target.dim}_temp${target.temperature}_eq1000_burn100000_steps1000_every10.h5
  test_samples_path: null


flow:                                  # Properties of the flow model
  solvent_flow: true                   # Bool, flag whether to use the flow model used for aldp in the paper
  use_snf: false                      # Bool, flag whether to use Stochastic Flow
  resampled_base: false               # Bool, flag whether to use resampled base distribution
  type: circ-coup-nsf                 # String, type of the flow: "circ-coup-nsf", "circ-ar-nsf"
  base:
    type: gauss
    params: null
    learn_mean_var: false             # Bool, flag whether to learn mean and variance of base distribution.
  blocks: 12                          # Int, number of Real NVP blocks, consisting of an ActNorm layer
  blocks_per_layer: 1                 # Int, number of blocks per layer
  hidden_units: 256                   # Int, number of hidden units of the NN in neural spline layers
  num_bins: 8                         # Int, number of bins of the neural splines
  init_identity: True                 # Bool, flag whether to initialize layers as identity map
  dropout: 0.                         # Float, dropout probability for the NN layers
                                      # For below: if specified, a permutation, and a affine coupling layer
  circ_shift: random                  # String, whether to shift circular coordinates, can be none, constant, or random
  actnorm: False                      # Bool, flag whether to include an ActNorm layers
  mixing: null                        # String, how features are mixed


fab:
  loss_type: forward_kl               # Whether to use AIS, and type of loss for AIS. If not AIS, rest of args are only used for potential eval.
  use_ais: false                      # Whether to use AIS for evaluation still
  n_intermediate_distributions: 8     # Int, number of intermediate distributions
  alpha: 2.0
  transition_operator:
    type: hmc
    n_inner_steps: 4
    init_step_size: 0.05              # original eps = 0.1 / 2
    tune_step_size: true              # Bool, flag whether to adjust step size for Metropolis
    target_p_accept: 0.65


training:                             # Properties of the training procedure
  seed: 0                             # Int, seed to be used for the random number generator
  num_workers: 4  # 4                 # Int, number of workers to be used for data loading
  checkpoint_load_dir: null           # Checkpoint dir to load from if continuing training
  n_iterations: 5000  #50000                 # Int, maximum number of iteration
  n_flow_forward_pass: null
  batch_size: 1024               # Int, batch size used during training
  tlimit: null                        # Int, time limit of run in seconds
  use_gpu: true
  use_64_bit: true
  lr: 5.e-4                           # Double, learning rate
  wd: 1.e-5                           # Double, regularization parameter
  optimizer: adam                     # String, name of the optimizer
  lr_scheduler:
    type: cosine                      # String, kind of LR scheduler, can be exponential, cosine
  warmup_iter: null                   # Int, number of iterations used for warmup
  max_grad_norm: 1.e3                 # Double, limit for gradient clipping
  use_buffer: false  # true                 # Buffer args follow below here
  prioritised_buffer: false  #true          # If True then FAB loss WILL be used no matter what fab.loss_type is.


evaluation:  # TODO
  n_plots: 20                         # number of times we visualise the model throughout training.
  n_eval: 50  # ${training.n_iterations}  # 20                          # for calculating metrics of flow w.r.t target.
  eval_batch_size: 1024              # For AIS-type evaluation, must be a multiple of inner batch size (`batch_size`)
  n_checkpoints: 10                   # number of model checkpoints saved
  save_path:  ./results/h2o_in_h2o/seed${training.seed}/  # Relative to /DEPLOYMENT_DIR/outputs/yyyy-mm-dd/HH-MM-SS/LOGGER_DIR_TREE/


logger:
  pandas_logger:
    save_period: 100                  # how often to save the pandas dataframe as a csv
#  wandb:
#    name: ${fab.loss_type}_pr${training.prioritised_buffer}_alpha${fab.alpha}
#    project: H2O_in_H2O
#    entity: timsey
#    tags: [fab_explore]


# TODO:
#  - check that code runs, and is similar enough to ALDP run.
#  - plotting
#  - evaluation
#  - logging of target distribution
#  - Are we using the correct flow? Do we indeed not have circular indices (no freely-rotating bonds).
#  - Check that our system is implemented well. Do we need to set any extra parameters for OpenMM? How do we make
#     sure that the indices match?
#  -